{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noteboook for testing view classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "projectdir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(projectdir, 'src'))\n",
    "\n",
    "from d00_utils.db_utils import dbReadWriteClassification, dbReadWriteViews, dbReadWriteClean\n",
    "import d02_intermediate.download_dcm as download_dcm\n",
    "import d03_classification.predict_views as predict_views\n",
    "from d00_utils.log_utils import setup_logging\n",
    "from d03_classification.evaluate_views import evaluate_views\n",
    "\n",
    "logger = setup_logging(__name__, 'notebook_logger')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'train_split100_downsampleby20'\n",
    "model_name = 'view_23_e5_class_11-Mar-2018'\n",
    "date_run = datetime.date(year = 2019, month = 8, day = 14)\n",
    "study_filter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluate_views(img_dir, model_name, date_run, study_filter=None, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_class = dbReadWriteClassification()\n",
    "io_class.get_table('evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = os.path.join(os.path.expanduser('~/data'),'02_intermediate', 'train_split100_downsampleby20')\n",
    "model_path=os.path.join(os.path.expanduser('~/models'), 'view_23_e5_class_11-Mar-2018')\n",
    "if_exists='replace'\n",
    "feature_dim=1\n",
    "label_dim = len(predict_views.view_classes)\n",
    "model_name = os.path.basename(model_path)\n",
    "\n",
    "probabilities = predict_views._classify(img_dir, feature_dim, label_dim, model_path)\n",
    "\n",
    "df_columns = ['output_' + x for x in predict_views.view_classes]\n",
    "df = (\n",
    "    pd.DataFrame.from_dict(probabilities, columns=df_columns, orient='index')\n",
    "    .rename_axis('file_name')\n",
    "    .reset_index()\n",
    ") \n",
    "df['file_name'] = df['file_name'].apply(lambda x: x.split('.')[0])\n",
    "df['study_id'] = df['file_name'].apply(lambda x: x.split('_')[1])\n",
    "df['model_name'] = model_name\n",
    "df['date_run'] = datetime.datetime.now()\n",
    "df['img_dir'] = os.path.basename(img_dir)\n",
    "cols = ['study_id','file_name','model_name','date_run','img_dir'] + df_columns\n",
    "df = df[cols]\n",
    "\n",
    "io_classification = dbReadWriteClassification()\n",
    "io_classification.save_to_db(df, 'test_probabilities_frames', if_exists)\n",
    "\n",
    "logger.info(\n",
    "    '{} prediction on frames with model {} (feature_dim={}).'.format(\n",
    "        os.path.basename(img_dir), model_name, feature_dim\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_class = dbReadWriteClassification()\n",
    "probabilities_frames = io_class.get_table('test_probabilities_frames')\n",
    "probabilities_frames['file_name'] = probabilities_frames['file_name'].apply(\n",
    "    lambda x: x.rsplit('_', 1)[0]\n",
    ")\n",
    "\n",
    "mean_cols = ['output_' + x for x in predict_views.view_classes]\n",
    "agg_cols = dict(zip(mean_cols, ['mean'] * len(mean_cols)))\n",
    "agg_cols['test_probabilities_frame_id'] = 'count'\n",
    "\n",
    "probabilities = (\n",
    "    probabilities_frames.groupby(['study_id','file_name','model_name','date_run','img_dir'])\n",
    "    .agg(agg_cols)\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "probabilities.rename(columns={'test_probabilities_frame_id': 'frame_count'}, inplace=True)\n",
    "\n",
    "io_class.save_to_db(probabilities, 'test_probabilities_instances', if_exists)\n",
    "logger.info('Aggregated probabilities saved to table classification.probabilities_instances')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = io_class.get_table('test_probabilities_instances')\n",
    "\n",
    "predictions = probabilities.drop(columns=['test_probabilities_instance_id', 'frame_count']\n",
    "                                          ).set_index(['study_id','file_name','model_name','date_run','img_dir'])\n",
    "predictions['view23_pred'] = predictions.idxmax(axis=1).apply(lambda x : x.split('_', 1)[1])\n",
    "predictions['view4_dev'] = predictions['view23_pred'].map(predict_views.maps_dev)\n",
    "predictions['view4_seg'] = predictions['view23_pred'].map(predict_views.maps_seg)\n",
    "\n",
    "df = predictions.loc[:,['view23_pred','view4_dev','view4_seg']]\n",
    "df.reset_index(inplace=True)     \n",
    "\n",
    "io_class.save_to_db(df, 'test_predictions', if_exists)    \n",
    "logger.info('Predicted views saved to table classification.predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels via views.instances_w_labels table\n",
    "io_views = dbReadWriteViews()\n",
    "io_class = dbReadWriteClassification()\n",
    "\n",
    "groundtruth = io_views.get_table('instances_w_labels')\n",
    "groundtruth.rename(columns={'filename': 'file_name','studyidk':'study_id','view': 'view_true'}, inplace=True)\n",
    "groundtruth['file_name'] = 'a_'+groundtruth['study_id'].astype(str) + '_' + groundtruth['file_name'].astype(str)\n",
    "groundtruth.drop(columns=['sopinstanceuid','instanceidk'], inplace=True)\n",
    "predictions = io_class.get_table('test_predictions')\n",
    "\n",
    "# Merge tables df_new and labels_df\n",
    "predict_truth = predictions.merge(groundtruth, on=['file_name', 'study_id'], how='left')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _groundtruth_views():\n",
    "\n",
    "    # Get ground truth labels via views.instances_w_labels table\n",
    "    io_views = dbReadWriteViews()\n",
    "    io_class = dbReadWriteClassification()\n",
    "\n",
    "    groundtruth = io_views.get_table(\"instances_w_labels\")\n",
    "    groundtruth.rename(\n",
    "        columns={\"filename\": \"file_name\", \"studyidk\": \"study_id\", \"view\": \"view_true\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    groundtruth[\"file_name\"] = (\n",
    "        \"a_\"\n",
    "        + groundtruth[\"study_id\"].astype(str)\n",
    "        + \"_\"\n",
    "        + groundtruth[\"file_name\"].astype(str)\n",
    "    )\n",
    "    groundtruth.drop(columns=[\"sopinstanceuid\", \"instanceidk\"], inplace=True)\n",
    "    predictions = io_class.get_table(\"test_predictions\")\n",
    "\n",
    "    # Merge tables df_new and labels_df\n",
    "    predict_truth = predictions.merge(groundtruth, on=[\"file_name\", \"study_id\"])\n",
    "\n",
    "    return predict_truth\n",
    "\n",
    "\n",
    "def evaluate_view_map(img_dir, model_name, date_run, view_mapping, study_filter=None):\n",
    "\n",
    "    predict_truth = _groundtruth_views()\n",
    "\n",
    "    df = predict_truth.loc[\n",
    "        (predict_truth[\"img_dir\"] == img_dir)\n",
    "        & (predict_truth[\"model_name\"] == model_name)\n",
    "        & (pd.to_datetime(predict_truth[\"date_run\"]).dt.date == date_run),\n",
    "        :,\n",
    "    ]\n",
    "\n",
    "    if type(study_filter) == dict:\n",
    "        df = df[df[\"study_id\"].isin(list(study_filter.values())[0])]\n",
    "        study_filter = list(study_filter.keys())[0]\n",
    "\n",
    "    mcm = multilabel_confusion_matrix(\n",
    "        y_pred=df[view_mapping],\n",
    "        y_true=df[\"view_true\"],\n",
    "        labels=[\"a2c\", \"a4c\", \"plax\", \"other\"],\n",
    "    )\n",
    "\n",
    "    df_mcm = pd.DataFrame(\n",
    "        np.reshape(mcm, (4, 4)) / np.sum(mcm[0]),\n",
    "        columns=[\"tn\", \"fp\", \"fn\", \"tp\"],\n",
    "        index=[\"a2c\", \"a4c\", \"plax\", \"other\"],\n",
    "    )\n",
    "\n",
    "    eval_out = df_mcm.rename_axis(\"view\").reset_index()\n",
    "\n",
    "    eval_out[\"model_name\"] = model_name\n",
    "    eval_out[\"img_dir\"] = img_dir\n",
    "    eval_out[\"date_run\"] = df[\"date_run\"][0]\n",
    "    eval_out[\"view_mapping\"] = view_mapping\n",
    "    eval_out[\"study_filter\"] = study_filter\n",
    "\n",
    "    cols = list(eval_out.columns[5::]) + list(eval_out.columns[:5])\n",
    "    eval_out = eval_out[cols]\n",
    "\n",
    "    return eval_out\n",
    "\n",
    "\n",
    "def test_evaluate_views(img_dir, model_name, date_run, study_filter=None, if_exists=\"append\"):\n",
    "    \"\"\"Evaluates classification.predictions table\n",
    "    \n",
    "    :param img_dir:\n",
    "    :param model_name:\n",
    "    :param date_run:\n",
    "    :param study_filter:\n",
    "    :param if_exists:            \n",
    "    \n",
    "    \"\"\"\n",
    "    for view_mapping in [\"view4_dev\", \"view4_seg\"]:\n",
    "        eval_out = evaluate_view_map(\n",
    "            img_dir, model_name, date_run, view_mapping, study_filter=None\n",
    "        )\n",
    "\n",
    "        io_class = dbReadWriteClassification()\n",
    "        io_class.save_to_db(eval_out, \"evaluation\", if_exists)\n",
    "\n",
    "        logger.info(\n",
    "            \"Evaluated {} {} {} {})\".format(img_dir, model_name, date_run, view_mapping)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_usal_echo)",
   "language": "python",
   "name": "conda_usal_echo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
