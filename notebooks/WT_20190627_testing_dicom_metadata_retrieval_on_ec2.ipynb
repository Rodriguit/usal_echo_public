{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental notebook for testing data retrieval from S3\n",
    "*author: Wiebke Toussaint*\n",
    "\n",
    "The functions tested in this notebook have been integrated into `usal_echo.d00_utils.xxx` and `usal_echo.d01_data.dicom_processing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links  \n",
    "[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#bucket)  \n",
    "[s3fs](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem.metadata)  \n",
    "[gdcmdump](http://gdcm.sourceforge.net/wiki/index.php/Gdcmdump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "projectdir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(projectdir,'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "all_objects = s3_client.list_objects_v2(Bucket = 'cibercv') \n",
    "all_files = [d['Key'] for d in all_objects['Contents']] #get a list of all files in bucket\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_bucket = s3_resource.Bucket('cibercv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_files = [f for f in all_files if f.endswith('.dcm')]\n",
    "print('all_files', len(all_files))\n",
    "print('dicom_files', len(dicom_files))\n",
    "dicom_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicom_metadata(bucket, file_path, description=False):\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    tmp = tempfile.NamedTemporaryFile()\n",
    "\n",
    "    # Dump metadata of file to temp file\n",
    "    s3.download_file(bucket, file_path, tmp.name)\n",
    "    os.system('gdcmdump '+ tmp.name +' > temp.txt')\n",
    "\n",
    "    dir_name = file_path.split('/')[0]\n",
    "    file_name = file_path.split('/')[1].split('.')[0]\n",
    "\n",
    "    # Parse temp.txt file to extract tags\n",
    "    temp_file='temp.txt'\n",
    "    meta = []\n",
    "    with open(temp_file, 'r') as f:\n",
    "        line_meta = []\n",
    "        for one_line in f:            \n",
    "            try:\n",
    "                clean_line = one_line.replace(']','').strip()\n",
    "                if not clean_line: # ignore empty lines\n",
    "                    continue\n",
    "                elif not clean_line.startswith('#'): # ignore comment lines:\n",
    "                    tag1 = clean_line[1:5]\n",
    "                    tag2 = clean_line[6:10]\n",
    "                    if description == False:\n",
    "                        value = clean_line[15:clean_line.find('#')].strip().replace('[','')\n",
    "                    elif description == True:\n",
    "                        value = clean_line[clean_line.find('#')+2:].strip()\n",
    "                    line_meta=[dir_name, file_name, tag1, tag2, value]\n",
    "                    meta.append(line_meta)\n",
    "            except IndexError:\n",
    "                break\n",
    "                    \n",
    "    df = pd.DataFrame.from_records(meta, columns=['dirname','filename','tag1','tag2','value'])\n",
    "    df_dedup = df.drop_duplicates(keep='first')\n",
    "    df_dedup_goodvals = df_dedup[~df_dedup.value.str.contains('no value')]\n",
    "    df_dedup_goodvals_short = df_dedup_goodvals[(df_dedup_goodvals['value'].str.len()>0)&(df_dedup_goodvals['value'].str.len()<50)]\n",
    "    \n",
    "    return df_dedup_goodvals_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://alexwlchan.net/2018/01/listing-s3-keys-redux/\n",
    "\n",
    "def get_matching_s3_objects(bucket, prefix='', suffix=''):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch objects whose key starts with\n",
    "        this prefix (optional).\n",
    "    :param suffix: Only fetch objects whose keys end with\n",
    "        this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # If the prefix is a single string (not a tuple of strings), we can\n",
    "    # do the filtering directly in the S3 API.\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # The S3 API response is a large blob of metadata.\n",
    "        # 'Contents' contains information about the listed objects.\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "\n",
    "        try:\n",
    "            contents = resp['Contents']\n",
    "        except KeyError:\n",
    "            return\n",
    "\n",
    "        for obj in contents:\n",
    "            key = obj['Key']\n",
    "            if key.startswith(prefix) and key.endswith(suffix):\n",
    "                yield obj\n",
    "\n",
    "        # The S3 API is paginated, returning up to 1000 keys at a time.\n",
    "        # Pass the continuation token into the next response, until we\n",
    "        # reach the final page (when this field is missing).\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix='', suffix=''):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    for obj in get_matching_s3_objects(bucket, prefix, suffix):\n",
    "        yield obj['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicom_metadata(df, metadata_file_name=None):\n",
    "\n",
    "    # Save metadata as csv file\n",
    "    data_path = os.path.join(os.path.expanduser('~'),'data_usal','02_intermediate')\n",
    "    os.makedirs(os.path.expanduser(data_path), exist_ok=True)\n",
    "    if metadata_file_name is None:\n",
    "        dicom_meta_path = os.path.join(data_path,'dicom_metadata.csv')\n",
    "    else:\n",
    "        dicom_meta_path = os.path.join(data_path,'dicom_metadata_'+str(metadata_file_name)+'.csv')\n",
    "    if not os.path.isfile(dicom_meta_path): # create new file if it does not exist\n",
    "        print('Creating new metadata file')\n",
    "        df.to_csv(dicom_meta_path, index=False)\n",
    "    else: # if file exists append\n",
    "        df.to_csv(dicom_meta_path, mode='a', index=False, header=False)\n",
    "               \n",
    "    print('dicom metadata saved for study {}, directory {}'.format(df.iloc[0,0], df.iloc[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3keys = get_matching_s3_keys('cibercv','100155','.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "for key in get_matching_s3_keys('cibercv','100000','.dcm'): \n",
    "    df = get_dicom_metadata('cibercv', key)\n",
    "    write_dicom_metadata(df, 'test')\n",
    "#os.remove('temp.txt')\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.read_csv(os.path.join(projectdir,'data','02_intermediate','dicom_metadata_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf['tags'] = list(zip(tf['tag1'],tf['tag2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[tf['tags']==('fffe','e000')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "s3_bucket = 's3://cibercv/' \n",
    "s3_studies = s3.ls(s3_bucket)\n",
    "s3_studies[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = s3.url(s3_study_paths[3])\n",
    "all_files = s3.ls(s3_study_paths[3])\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = get_dicom_metadata(dirpath, s3_study_paths[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('gdcmdump '+ dirpath +' > temp.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dssg",
   "language": "python",
   "name": "dssg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
